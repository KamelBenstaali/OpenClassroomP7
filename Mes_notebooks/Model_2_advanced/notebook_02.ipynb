{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1fefa3",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ab3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "import swifter\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fe1c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Version TF : 2.20.0\n",
      "CUDA dispo dans la build : True\n",
      "GPUs vus par TensorFlow : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Version TF :\", tf.__version__)\n",
    "print(\"CUDA dispo dans la build :\", tf.test.is_built_with_cuda())\n",
    "print(\"GPUs vus par TensorFlow :\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bba34f",
   "metadata": {},
   "source": [
    "# DATA READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd2828b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"../../sentiment140/training.1600000.processed.noemoticon.csv\",\n",
    "                 encoding='latin-1',\n",
    "                 header=None,\n",
    "                 names=['sentiment','id','date','query','user','tweet'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "382f0304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['sentiment','tweet']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dddef0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@xnausikaax oh no! where did u order from? tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A great hard training weekend is over.  a coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Right, off to work  Only 5 hours to go until I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I am craving for japanese food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Jean Michel Jarre concert tomorrow  gotta work...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0          0  @xnausikaax oh no! where did u order from? tha...\n",
       "1          0  A great hard training weekend is over.  a coup...\n",
       "2          0  Right, off to work  Only 5 hours to go until I...\n",
       "3          0                    I am craving for japanese food \n",
       "4          0  Jean Michel Jarre concert tomorrow  gotta work..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stratified sampling to have tweets of the two sentiments\n",
    "df_negatifs = df[df['sentiment'] == 0].sample(8000, random_state=42)\n",
    "df_positifs = df[df['sentiment'] == 4].sample(8000, random_state=42)\n",
    "\n",
    "# So we get a sample of 10% of the original dataset\n",
    "df = pd.concat([df_negatifs, df_positifs]).reset_index(drop=True)\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03524548",
   "metadata": {},
   "source": [
    "# CLEANING TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe9916",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54aa3c3",
   "metadata": {},
   "source": [
    "âœ… Preprocessing steps(preparing for TF-IDF + Logistic Regression)\n",
    "\n",
    "Compared to the preprocessing of tweets made during the application of logistic regression, the steps written in red have been removed, and the steps in green are repeated or added for this time.\n",
    "\n",
    "\n",
    "<span style=\"color:green\"></span>\n",
    "\n",
    "<span style=\"color:green\">- Lowercase</span>\n",
    "\n",
    "<span style=\"color:red\">- Expand contractions</span>\n",
    "\n",
    "<span style=\"color:green\">- Convert emoticons â†’ words</span>\n",
    "\n",
    "<span style=\"color:green\">- Convert emojis â†’ words</span>\n",
    "\n",
    "<span style=\"color:green\">- Remove URLs, mentions, hashtags</span>\n",
    "\n",
    "<span style=\"color:red\">- special chars, and punctuation</span>\n",
    "\n",
    "<span style=\"color:green\">- Tokenize</span>\n",
    "\n",
    "<span style=\"color:red\">- Remove stopwords</span>\n",
    "\n",
    "<span style=\"color:red\">- Lemmatize</span>\n",
    "\n",
    "<span style=\"color:red\">- Join tokens back</span>\n",
    "\n",
    "<span style=\"color:green\">- Build vocab (+ OOV token), convert to integer sequences.</span>\n",
    "\n",
    "<span style=\"color:green\">- Pad/trim to a fixed max length; create attention masks if you use masking.</span>\n",
    "\n",
    "<span style=\"color:green\">- Initialize an Embedding layer (trainable or with pretrained vectors like GloVe-Twitter/fastText); then LSTM/biLSTM â†’ classifier.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0947b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "try:\n",
    "    # optional but nice: converts emojis â†’ words like \":smiling_face:\" -> \"smiling face\"\n",
    "    from emoji import demojize\n",
    "    _HAS_EMOJI = True\n",
    "except Exception:\n",
    "    _HAS_EMOJI = False\n",
    "\n",
    "# ---------- light, meaning-preserving normalization ----------\n",
    "\n",
    "_EMOTICONS = {\n",
    "    r\":-\\)|:\\)|=\\)|:\\]\": \"smile\",\n",
    "    r\":-D|:D|=D\": \"laugh\",\n",
    "    r\":-\\(|:\\(|=\\(|:\\[\": \"sad\",\n",
    "    r\":'\\(|:'-\\(\": \"cry\",\n",
    "    r\";-\\)|;\\)\": \"wink\",\n",
    "    r\":-P|:P\": \"playful\",\n",
    "    r\":/|:-/\": \"skeptical\",\n",
    "    r\":\\*\": \"kiss\",\n",
    "    r\">:\\(|>:-\\(\": \"angry\",\n",
    "    r\"XD|xD\": \"laugh\",\n",
    "}\n",
    "\n",
    "_EMOTICON_REGEXES = [(re.compile(p), w) for p, w in _EMOTICONS.items()]\n",
    "\n",
    "_URL_RE   = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "_USER_RE  = re.compile(r\"@\\w+\")\n",
    "_NUM_RE   = re.compile(r\"\\b\\d+\\b\")\n",
    "# keep ! and ?; drop most other punctuation later if you want (we keep them)\n",
    "# Hashtags: keep the hashtag and add its content as a separate token\n",
    "_HASHTAG_RE = re.compile(r\"#(\\w+)\")\n",
    "# compress character repetitions to max 3 (so \"sooooo\" -> \"sooo\")\n",
    "_REPEAT_RE  = re.compile(r\"(.)\\1{3,}\")\n",
    "\n",
    "def _emoticons_to_words(text: str) -> str:\n",
    "    for rgx, word in _EMOTICON_REGEXES:\n",
    "        text = rgx.sub(f\" {word} \", text)\n",
    "    return text\n",
    "\n",
    "def _emojis_to_words(text: str) -> str:\n",
    "    if not _HAS_EMOJI:\n",
    "        return text\n",
    "    text = demojize(text, language=\"en\")\n",
    "    # demojize yields \":grinning_face_with_big_eyes:\" â†’ turn to words\n",
    "    text = re.sub(r\":([a-zA-Z0-9_]+):\", lambda m: \" \" + m.group(1).replace(\"_\", \" \") + \" \", text)\n",
    "    return text\n",
    "\n",
    "def normalize_tweet(t: str) -> str:\n",
    "    t = t.strip().lower()\n",
    "    t = _URL_RE.sub(\" <URL> \", t)\n",
    "    t = _USER_RE.sub(\" <USER> \", t)\n",
    "    t = _NUM_RE.sub(\" <NUM> \", t)\n",
    "    t = _emoticons_to_words(t)\n",
    "    t = _emojis_to_words(t)\n",
    "    # keep hashtag token, also add its de-hashed word\n",
    "    t = _HASHTAG_RE.sub(lambda m: f\" #{m.group(1)} {m.group(1)} \", t)\n",
    "    # compress extreme elongations but keep emphasis\n",
    "    t = _REPEAT_RE.sub(r\"\\1\\1\\1\", t)\n",
    "    # normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "# ---------- training-time helpers ----------\n",
    "def build_tokenizer(train_texts: List[str],\n",
    "                    vocab_size: int = 20000,\n",
    "                    oov_token: str = \"<OOV>\") -> Tokenizer:\n",
    "    \"\"\"Fit a Keras tokenizer on *normalized* training texts.\"\"\"\n",
    "    norm_train = [normalize_tweet(t) for t in train_texts]\n",
    "    tok = Tokenizer(num_words=vocab_size, oov_token=oov_token, filters=\"\")  # keep punctuation like ! ?\n",
    "    tok.fit_on_texts(norm_train)\n",
    "    return tok\n",
    "\n",
    "def preprocess_train(train_texts: List[str],\n",
    "                     tokenizer: Tokenizer,\n",
    "                     max_len: int = 50) -> Tuple[List[List[int]], List[List[int]]]:\n",
    "    \"\"\"Return padded sequences and (optional) attention masks for training.\"\"\"\n",
    "    norm = [normalize_tweet(t) for t in train_texts]\n",
    "    seqs = tokenizer.texts_to_sequences(norm)\n",
    "    padded = pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    # attention mask: 1 for real tokens, 0 for padding (can be used with Masking)\n",
    "    masks = (padded != 0).astype(\"int32\").tolist()\n",
    "    return padded, masks\n",
    "\n",
    "def preprocess_test(test_texts: List[str],\n",
    "                    tokenizer: Tokenizer,\n",
    "                    max_len: int) -> Tuple[List[List[int]], List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Normalize tweets, convert to integer sequences using an already-fitted tokenizer,\n",
    "    and pad/trim to max_len. Returns (padded_sequences, attention_masks).\n",
    "    \"\"\"\n",
    "    norm = [normalize_tweet(t) for t in test_texts]\n",
    "    seqs = tokenizer.texts_to_sequences(norm)\n",
    "    padded = pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    masks = (padded != 0).astype(\"int32\").tolist()\n",
    "    return padded, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a1580",
   "metadata": {},
   "source": [
    "## Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3dfc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "import mlflow\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# ---------- constants ----------\n",
    "MAX_LEN    = 80\n",
    "VOCAB_SIZE = 20000   # tokenizer + embedding vocab size\n",
    "\n",
    "# ---------- data ----------\n",
    "texts = df[\"tweet\"].astype(str).tolist()\n",
    "labels = df[\"sentiment\"].astype(\"float32\").to_numpy()\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.5, random_state=42, stratify=y_tmp\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Val size:\", len(X_val), \"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957ef19",
   "metadata": {},
   "source": [
    "## Griding optuna + Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eeb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- MLflow experiment ----------\n",
    "mlflow.set_experiment(\"tweet_bilstm_w2v_optuna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c096d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Unique labels in full data:\", np.unique(labels, return_counts=True))\n",
    "\n",
    "print(\"Train:\", np.unique(y_train, return_counts=True))\n",
    "print(\"Val:  \", np.unique(y_val,   return_counts=True))\n",
    "print(\"Test: \", np.unique(y_test,  return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf24074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_params(params, tokenizer, w2v, max_len=MAX_LEN):\n",
    "    embed_dim     = params[\"embed_dim\"]\n",
    "    lstm_units    = params[\"lstm_units\"]\n",
    "    dense_units   = params[\"dense_units\"]\n",
    "    dropout_rate  = params[\"dropout_rate\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    trainable_emb = params[\"trainable_embedding\"]\n",
    "\n",
    "    # ---- num_words consistent with tokenizer ----\n",
    "    num_words = tokenizer.num_words\n",
    "    if num_words is None:\n",
    "        num_words = min(VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
    "\n",
    "    # ---- embedding matrix ----\n",
    "    embedding_matrix = np.zeros((num_words, embed_dim), dtype=\"float32\")\n",
    "\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx >= num_words:\n",
    "            continue\n",
    "        if word in w2v.wv:\n",
    "            vec = w2v.wv[word]\n",
    "            if vec.shape[0] == embed_dim:\n",
    "                embedding_matrix[idx] = vec\n",
    "\n",
    "    # ---- model ----\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=num_words,\n",
    "            output_dim=embed_dim,\n",
    "            weights=[embedding_matrix],\n",
    "            mask_zero=True,\n",
    "            trainable=trainable_emb,\n",
    "        ),\n",
    "        Bidirectional(LSTM(lstm_units)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation=\"relu\"),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation=\"sigmoid\")  # binary sentiment\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c20f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # ---- sample params ----\n",
    "    params = {\n",
    "        \"embed_dim\": trial.suggest_categorical(\"embed_dim\", [100, 200, 300]),\n",
    "        \"w2v_window\": trial.suggest_int(\"w2v_window\", 3, 7),\n",
    "        \"w2v_min_count\": trial.suggest_int(\"w2v_min_count\", 1, 5),\n",
    "        \"w2v_sg\": trial.suggest_categorical(\"w2v_sg\", [0, 1]),\n",
    "        \"w2v_epochs\": trial.suggest_int(\"w2v_epochs\", 5, 15),\n",
    "        \"lstm_units\": trial.suggest_int(\"lstm_units\", 64, 256, step=64),\n",
    "        \"dense_units\": trial.suggest_int(\"dense_units\", 32, 128, step=32),\n",
    "        \"dropout_rate\": trial.suggest_float(\"dropout_rate\", 0.1, 0.5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True),\n",
    "        \"trainable_embedding\": trial.suggest_categorical(\"trainable_embedding\", [False, True]),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 3, 8),\n",
    "    }\n",
    "\n",
    "    # ---- CHILD RUN ----\n",
    "    with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # tokenizer\n",
    "        tok = build_tokenizer(X_train, vocab_size=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "\n",
    "        # word2vec\n",
    "        train_tokens = [normalize_tweet(t).split() for t in X_train]\n",
    "        w2v = Word2Vec(\n",
    "            sentences=train_tokens,\n",
    "            vector_size=params[\"embed_dim\"],\n",
    "            window=params[\"w2v_window\"],\n",
    "            min_count=params[\"w2v_min_count\"],\n",
    "            workers=4,\n",
    "            sg=params[\"w2v_sg\"],\n",
    "            negative=10,\n",
    "            epochs=params[\"w2v_epochs\"],\n",
    "        )\n",
    "\n",
    "        # sequences\n",
    "        X_train_pad, _ = preprocess_train(X_train, tokenizer=tok, max_len=MAX_LEN)\n",
    "        X_val_pad,   _ = preprocess_test(X_val, tokenizer=tok, max_len=MAX_LEN)\n",
    "\n",
    "        # model\n",
    "        model = build_model_with_params(params, tokenizer=tok, w2v=w2v, max_len=MAX_LEN)\n",
    "\n",
    "        # dataset\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train_pad, y_train)).shuffle(10000).batch(params[\"batch_size\"])\n",
    "        val_ds   = tf.data.Dataset.from_tensor_slices((X_val_pad, y_val)).batch(params[\"batch_size\"])\n",
    "\n",
    "        history = model.fit(train_ds, validation_data=val_ds, epochs=params[\"epochs\"], verbose=0)\n",
    "\n",
    "        # ---- VALIDATION PREDICTIONS ----\n",
    "        y_pred_prob = model.predict(X_val_pad, verbose=0).ravel()\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_val, y_pred, average=\"binary\", zero_division=0)\n",
    "\n",
    "        # log aggregated metrics\n",
    "        mlflow.log_metric(\"val_accuracy\", acc)\n",
    "        mlflow.log_metric(\"val_precision\", prec)\n",
    "        mlflow.log_metric(\"val_recall\", rec)\n",
    "        mlflow.log_metric(\"val_f1\", f1)\n",
    "\n",
    "        # ðŸ‘‰ Optuna will maximize F1\n",
    "        return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52451188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred_prob = self.model.predict(self.X_val, verbose=0).ravel()\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "        acc = accuracy_score(self.y_val, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(self.y_val, y_pred, average=\"binary\", zero_division=0)\n",
    "\n",
    "        mlflow.log_metric(\"val_accuracy_trace\", acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_precision_trace\", prec, step=epoch)\n",
    "        mlflow.log_metric(\"val_recall_trace\", rec, step=epoch)\n",
    "        mlflow.log_metric(\"val_f1_trace\", f1, step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"optuna_bilstm_w2v_parent\"):\n",
    "\n",
    "    # ---- 1) Run Optuna ----\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_params = best_trial.params\n",
    "\n",
    "    print(\"Best val accuracy:\", best_trial.value)\n",
    "    print(\"Best params:\", best_params)\n",
    "\n",
    "    # Log best params at parent level\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
    "\n",
    "    # full data\n",
    "    X_train_full = X_train + X_val\n",
    "    y_train_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "    tok_full = build_tokenizer(X_train_full, vocab_size=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokens_full = [normalize_tweet(t).split() for t in X_train_full]\n",
    "\n",
    "    w2v_full = Word2Vec(\n",
    "        sentences=tokens_full,\n",
    "        vector_size=best_params[\"embed_dim\"],\n",
    "        window=best_params[\"w2v_window\"],\n",
    "        min_count=best_params[\"w2v_min_count\"],\n",
    "        workers=4,\n",
    "        sg=best_params[\"w2v_sg\"],\n",
    "        negative=10,\n",
    "        epochs=best_params[\"w2v_epochs\"],\n",
    "    )\n",
    "\n",
    "    X_train_full_pad, _ = preprocess_train(X_train_full, tokenizer=tok_full, max_len=MAX_LEN)\n",
    "    X_test_pad, _ = preprocess_test(X_test, tokenizer=tok_full, max_len=MAX_LEN)\n",
    "\n",
    "    model_best = build_model_with_params(best_params, tokenizer=tok_full, w2v=w2v_full)\n",
    "\n",
    "    cb = ValMetricsCallback(X_val=X_test_pad, y_val=y_test)  # tracing on val or test? choose val\n",
    "\n",
    "    train_full_ds = tf.data.Dataset.from_tensor_slices((X_train_full_pad, y_train_full)).shuffle(10000).batch(best_params[\"batch_size\"])\n",
    "\n",
    "    model_best.fit(train_full_ds, epochs=best_params[\"epochs\"], callbacks=[cb], verbose=0)\n",
    "\n",
    "    # FINAL TEST METRICS\n",
    "    y_test_pred_prob = model_best.predict(X_test_pad).ravel()\n",
    "    y_test_pred = (y_test_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec, test_rec, test_f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average=\"binary\")\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_precision\", test_prec)\n",
    "    mlflow.log_metric(\"test_recall\", test_rec)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6645378",
   "metadata": {},
   "source": [
    "## Griding optuna + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# =============================================================================\n",
    "# PREPROCESSING (your existing functions)\n",
    "# =============================================================================\n",
    "\n",
    "# (I keep your normalization as-is, not repeated here to save space)\n",
    "# Paste your normalize_tweet(), build_tokenizer(),\n",
    "# preprocess_train(), preprocess_test() functions here.\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD GLOVE EMBEDDINGS\n",
    "# =============================================================================\n",
    "\n",
    "def load_glove_embeddings(glove_path):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings[word] = vector\n",
    "\n",
    "    print(f\"Loaded {len(embeddings)} GloVe word vectors.\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def build_glove_embedding_matrix(tokenizer, glove_vectors, embed_dim, vocab_size):\n",
    "    num_words = min(vocab_size, len(tokenizer.word_index) + 1)\n",
    "    embedding_matrix = np.zeros((num_words, embed_dim), dtype=\"float32\")\n",
    "\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx >= num_words:\n",
    "            continue\n",
    "        vec = glove_vectors.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL BUILDER (uses GloVe embedding matrix)\n",
    "# =============================================================================\n",
    "\n",
    "def build_model_with_params(params, tokenizer, embedding_matrix, max_len, vocab_size):\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=min(vocab_size, len(tokenizer.word_index) + 1),\n",
    "            output_dim=params[\"embed_dim\"],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            mask_zero=True,\n",
    "            trainable=params[\"trainable_embedding\"]\n",
    "        ),\n",
    "        SpatialDropout1D(params[\"dropout_rate\"]),\n",
    "        Bidirectional(LSTM(params[\"lstm_units\"], return_sequences=False)),\n",
    "        Dense(params[\"dense_units\"], activation=\"relu\"),\n",
    "        Dropout(params[\"dropout_rate\"]),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(params[\"learning_rate\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CALLBACK FOR VAL METRIC TRACES (parent run)\n",
    "# =============================================================================\n",
    "\n",
    "class ValMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred_prob = self.model.predict(self.X_val, verbose=0).ravel()\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "        acc = accuracy_score(self.y_val, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            self.y_val, y_pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"val_accuracy_trace\", acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_precision_trace\", prec, step=epoch)\n",
    "        mlflow.log_metric(\"val_recall_trace\", rec, step=epoch)\n",
    "        mlflow.log_metric(\"val_f1_trace\", f1, step=epoch)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OPTUNA OBJECTIVE (child runs)\n",
    "# =============================================================================\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "MAX_LEN = 50\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # ---- Optuna Hyperparameters ----\n",
    "    params = {\n",
    "        \"embed_dim\": trial.suggest_categorical(\"embed_dim\", [50, 100, 200]),\n",
    "        \"lstm_units\": trial.suggest_int(\"lstm_units\", 64, 256, step=64),\n",
    "        \"dense_units\": trial.suggest_int(\"dense_units\", 32, 128, step=32),\n",
    "        \"dropout_rate\": trial.suggest_float(\"dropout_rate\", 0.1, 0.5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True),\n",
    "        \"trainable_embedding\": trial.suggest_categorical(\"trainable_embedding\", [False, True]),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 3, 8),\n",
    "    }\n",
    "\n",
    "    # ---- Child MLflow Run ----\n",
    "    with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # ---- Tokenizer ----\n",
    "        tok = build_tokenizer(X_train, VOCAB_SIZE, \"<OOV>\")\n",
    "\n",
    "        # ---- Load GloVe ----\n",
    "        glove_path = f\"embeddings/glove/glove.twitter.27B.{params['embed_dim']}d.txt\"\n",
    "        glove_vectors = load_glove_embeddings(glove_path)\n",
    "\n",
    "        # ---- Embedding Matrix ----\n",
    "        embedding_matrix = build_glove_embedding_matrix(tok, glove_vectors,\n",
    "                                                        params[\"embed_dim\"], VOCAB_SIZE)\n",
    "\n",
    "        # ---- Sequences ----\n",
    "        X_train_pad, _ = preprocess_train(X_train, tok, MAX_LEN)\n",
    "        X_val_pad, _ = preprocess_test(X_val, tok, MAX_LEN)\n",
    "\n",
    "        # ---- Model ----\n",
    "        model = build_model_with_params(params, tok, embedding_matrix, MAX_LEN, VOCAB_SIZE)\n",
    "\n",
    "        # ---- Datasets ----\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train_pad, y_train)) \\\n",
    "            .shuffle(10000).batch(params[\"batch_size\"])\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val_pad, y_val)) \\\n",
    "            .batch(params[\"batch_size\"])\n",
    "\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=params[\"epochs\"],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # ---- Validation Predictions ----\n",
    "        y_pred_prob = model.predict(X_val_pad, verbose=0).ravel()\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_val, y_pred,\n",
    "                                                           average=\"binary\", zero_division=0)\n",
    "\n",
    "        mlflow.log_metric(\"val_accuracy\", acc)\n",
    "        mlflow.log_metric(\"val_precision\", prec)\n",
    "        mlflow.log_metric(\"val_recall\", rec)\n",
    "        mlflow.log_metric(\"val_f1\", f1)\n",
    "\n",
    "        return f1  # maximize val F1\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN OPTUNA + PARENT MLFLOW LOGGING\n",
    "# =============================================================================\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best params:\", study.best_trial.params)\n",
    "print(\"Best val F1:\", study.best_trial.value)\n",
    "\n",
    "# =============================================================================\n",
    "# PARENT RUN FINAL MODEL (train on train+val, test on test)\n",
    "# =============================================================================\n",
    "\n",
    "with mlflow.start_run(run_name=\"optuna_bilstm_GloVe_parent\"):\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
    "\n",
    "    # ---- Combine Train + Val ----\n",
    "    X_train_full = X_train + X_val\n",
    "    y_train_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "    # ---- Tokenizer ----\n",
    "    tok_full = build_tokenizer(X_train_full, VOCAB_SIZE, \"<OOV>\")\n",
    "\n",
    "    # ---- Load GloVe ----\n",
    "    glove_path = f\"glove.twitter.27B.{best_params['embed_dim']}d.txt\"\n",
    "    glove_vectors = load_glove_embeddings(glove_path)\n",
    "\n",
    "    # ---- Embedding Matrix ----\n",
    "    embedding_matrix_full = build_glove_embedding_matrix(tok_full, glove_vectors,\n",
    "                                                         best_params[\"embed_dim\"], VOCAB_SIZE)\n",
    "\n",
    "    # ---- Sequences ----\n",
    "    X_train_full_pad, _ = preprocess_train(X_train_full, tok_full, MAX_LEN)\n",
    "    X_test_pad, _ = preprocess_test(X_test, tok_full, MAX_LEN)\n",
    "\n",
    "    # ---- Final Model ----\n",
    "    model_best = build_model_with_params(best_params, tok_full,\n",
    "                                         embedding_matrix_full, MAX_LEN, VOCAB_SIZE)\n",
    "\n",
    "    # ---- Callback (Validation Trace on Test Set) ----\n",
    "    cb = ValMetricsCallback(X_test_pad, y_test)\n",
    "\n",
    "    train_full_ds = tf.data.Dataset.from_tensor_slices((X_train_full_pad, y_train_full)) \\\n",
    "        .shuffle(10000).batch(best_params[\"batch_size\"])\n",
    "\n",
    "    model_best.fit(train_full_ds, epochs=best_params[\"epochs\"],\n",
    "                   callbacks=[cb], verbose=0)\n",
    "\n",
    "    # ---- Test Metrics ----\n",
    "    y_pred_prob = model_best.predict(X_test_pad).ravel()\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    test_prec, test_rec, test_f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_precision\", test_prec)\n",
    "    mlflow.log_metric(\"test_recall\", test_rec)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "\n",
    "    print(\"\\nFinal Test results:\")\n",
    "    print(\"Accuracy:\", test_acc)\n",
    "    print(\"Precision:\", test_prec)\n",
    "    print(\"Recall:\", test_rec)\n",
    "    print(\"F1:\", test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b67ad",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.p7venv)",
   "language": "python",
   "name": "p7venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
