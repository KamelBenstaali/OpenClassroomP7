{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d37da7d",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e63d7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/19 11:35:05 INFO mlflow.tracking.fluent: Experiment with name 'Model_3_USE' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "CUDA built: True\n"
     ]
    }
   ],
   "source": [
    "# START\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = \"Model_3_USE\"\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n",
    "print(\"CUDA built:\", tf.test.is_built_with_cuda())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447326e",
   "metadata": {},
   "source": [
    "# DATA READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84d1ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA READING\n",
    "df = pd.read_csv(\n",
    "    \"../../sentiment140/training.1600000.processed.noemoticon.csv\",\n",
    "    encoding=\"latin-1\",\n",
    "    header=None,\n",
    "    names=[\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"tweet\"],\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c55f7ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@xnausikaax oh no! where did u order from? tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A great hard training weekend is over.  a coup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Right, off to work  Only 5 hours to go until I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I am craving for japanese food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Jean Michel Jarre concert tomorrow  gotta work...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0          0  @xnausikaax oh no! where did u order from? tha...\n",
       "1          0  A great hard training weekend is over.  a coup...\n",
       "2          0  Right, off to work  Only 5 hours to go until I...\n",
       "3          0                    I am craving for japanese food \n",
       "4          0  Jean Michel Jarre concert tomorrow  gotta work..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only sentiment + tweet\n",
    "\n",
    "df = df[[\"sentiment\", \"tweet\"]]\n",
    "\n",
    "# Stratified sampling to have tweets of the two sentiments\n",
    "df_negatifs = df[df[\"sentiment\"] == 0].sample(8000, random_state=42)\n",
    "df_positifs = df[df[\"sentiment\"] == 4].sample(8000, random_state=42)\n",
    "\n",
    "# So we get a sample of 10% of the original dataset\n",
    "df = pd.concat([df_negatifs, df_positifs]).reset_index(drop=True)\n",
    "\n",
    "# Map labels 4 -> 1 for binary classification\n",
    "df[\"sentiment\"] = df[\"sentiment\"].replace(4, 1)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40ec371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    8000\n",
       "1    8000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class balance check\n",
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f043e",
   "metadata": {},
   "source": [
    "# CLEANING TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3c718",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43660c29",
   "metadata": {},
   "source": [
    "✅ Preprocessing steps(preparing for TF-IDF + Logistic Regression)\n",
    "\n",
    "Compared to the preprocessing of tweets made during the application of logistic regression, the steps written in red have been removed, and the steps in green are repeated or added for this time.\n",
    "\n",
    "\n",
    "<span style=\"color:green\">- Lowercase</span>\n",
    "\n",
    "<span style=\"color:green\">- Convert emoticons → words</span>\n",
    "\n",
    "<span style=\"color:green\">- Convert emojis → words</span>\n",
    "\n",
    "<span style=\"color:green\">- Remove URLs, mentions, hashtags</span>\n",
    "\n",
    "<span style=\"color:red\">- Tokenize</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754f472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING TEXT – light, meaning-preserving normalization\n",
    "\n",
    "try:\n",
    "    # optional but nice: converts emojis → words like \":smiling_face:\" -> \"smiling face\"\n",
    "    from emoji import demojize\n",
    "    _HAS_EMOJI = True\n",
    "except Exception:\n",
    "    _HAS_EMOJI = False\n",
    "\n",
    "# ---------- regexes & mappings ----------\n",
    "\n",
    "_EMOTICONS = {\n",
    "    r\":-\\)|:\\)|=\\)|:\\]\": \"smile\",\n",
    "    r\":-D|:D|=D\": \"laugh\",\n",
    "    r\":-\\(|:\\(|=\\(|:\\[\": \"sad\",\n",
    "    r\":'\\(|:'-\\(\": \"cry\",\n",
    "    r\";-\\)|;\\)\": \"wink\",\n",
    "    r\":-P|:P\": \"playful\",\n",
    "    r\":/|:-/\": \"skeptical\",\n",
    "    r\":\\*\": \"kiss\",\n",
    "    r\">:\\(|>:-\\(\": \"angry\",\n",
    "    r\"XD|xD\": \"laugh\",\n",
    "}\n",
    "\n",
    "_EMOTICON_REGEXES = [(re.compile(p), w) for p, w in _EMOTICONS.items()]\n",
    "\n",
    "_URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "_USER_RE = re.compile(r\"@\\w+\")\n",
    "_NUM_RE = re.compile(r\"\\b\\d+\\b\")\n",
    "# Hashtags: keep the hashtag and add its content as a separate token\n",
    "_HASHTAG_RE = re.compile(r\"#(\\w+)\")\n",
    "# compress character repetitions to max 3 (so \"sooooo\" -> \"sooo\")\n",
    "_REPEAT_RE = re.compile(r\"(.)\\1{3,}\")\n",
    "\n",
    "\n",
    "def _emoticons_to_words(text: str) -> str:\n",
    "    for rgx, word in _EMOTICON_REGEXES:\n",
    "        text = rgx.sub(f\" {word} \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def _emojis_to_words(text: str) -> str:\n",
    "    if not _HAS_EMOJI:\n",
    "        return text\n",
    "    text = demojize(text, language=\"en\")\n",
    "    # demojize yields \":grinning_face_with_big_eyes:\" → turn to words\n",
    "    text = re.sub(\n",
    "        r\":([a-zA-Z0-9_]+):\",\n",
    "        lambda m: \" \" + m.group(1).replace(\"_\", \" \") + \" \",\n",
    "        text,\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_tweet(t: str) -> str:\n",
    "    t = t.strip().lower()\n",
    "    t = _URL_RE.sub(\" <URL> \", t)\n",
    "    t = _USER_RE.sub(\" <USER> \", t)\n",
    "    t = _NUM_RE.sub(\" <NUM> \", t)\n",
    "    t = _emoticons_to_words(t)\n",
    "    t = _emojis_to_words(t)\n",
    "    # keep hashtag token, also add its de-hashed word\n",
    "    t = _HASHTAG_RE.sub(lambda m: f\" #{m.group(1)} {m.group(1)} \", t)\n",
    "    # compress extreme elongations but keep emphasis\n",
    "    t = _REPEAT_RE.sub(r\"\\1\\1\\1\", t)\n",
    "    # normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575da5e",
   "metadata": {},
   "source": [
    "## Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4447dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 12800 Val size: 1600 Test size: 1600\n"
     ]
    }
   ],
   "source": [
    "# Spliting – same logic as in Model 2\n",
    "\n",
    "texts = df[\"tweet\"].astype(str).tolist()\n",
    "labels = df[\"sentiment\"].astype(\"float32\").to_numpy()\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    texts,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp,\n",
    "    y_tmp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_tmp,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Val size:\", len(X_val), \"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91f0e1",
   "metadata": {},
   "source": [
    "## Precompute normalized texts for USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160a9ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exam return day, overall average of <NUM> . <NUM> % work harder next time crystal and no slack. astronomy night tonight and team dinner tomorrow!',\n",
       " \"out of school. i'm going to miss everyone so much! &lt; <NUM>\",\n",
       " '<USER>',\n",
       " \"i still hear a kitten meowing! i hope someone helps the poor thing. (i can't find it, it's probably on the other side of the fence.)\",\n",
       " \"stuffed peppers w/ spanish rice, beef, mushrooms, tomaters, acorn squash, &amp; onions - topped w/ cheese. if i'm eating leftovers all week..\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precompute normalized texts for USE (Applying the preprocessing functions)\n",
    "\n",
    "X_train_norm = [normalize_tweet(t) for t in X_train]\n",
    "X_val_norm = [normalize_tweet(t) for t in X_val]\n",
    "X_test_norm = [normalize_tweet(t) for t in X_test]\n",
    "\n",
    "# Quick sanity check\n",
    "X_train_norm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b03e729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading USE encoder...\n",
      "USE embedding dimension: 512\n"
     ]
    }
   ],
   "source": [
    "# Encode normalized texts with USE once to get dense features\n",
    "\n",
    "USE_URL = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "print(\"Loading USE encoder...\")\n",
    "use_encoder = hub.load(USE_URL)\n",
    "\n",
    "def encode_with_use(texts, batch_size=256):\n",
    "    vectors = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[start:start + batch_size]\n",
    "        embeddings = use_encoder(batch_texts)\n",
    "        vectors.append(embeddings.numpy())\n",
    "    return np.vstack(vectors)\n",
    "\n",
    "X_train_vec = encode_with_use(X_train_norm)\n",
    "X_val_vec = encode_with_use(X_val_norm)\n",
    "X_test_vec = encode_with_use(X_test_norm)\n",
    "\n",
    "USE_EMBED_DIM = X_train_vec.shape[1]\n",
    "print(\"USE embedding dimension:\", USE_EMBED_DIM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef8695",
   "metadata": {},
   "source": [
    "# Optuna + USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbedb84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 11:35:15,931] A new study created in memory with name: no-name-99cb70b6-beed-41b3-947e-242aba1c47df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-19 11:35:20,046] Trial 0 finished with value: 0.766390833863781 and parameters: {'dense_units': 192, 'dropout_rate': 0.23284310393964414, 'learning_rate': 0.00012221993428715677, 'batch_size': 128, 'epochs': 5}. Best is trial 0 with value: 0.766390833863781.\n",
      "2025-11-19 11:35:20.925262: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-19 11:35:21.405770: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_993', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "[I 2025-11-19 11:35:27,321] Trial 1 finished with value: 0.7793840351979887 and parameters: {'dense_units': 128, 'dropout_rate': 0.4109096958621382, 'learning_rate': 0.00018504668787497987, 'batch_size': 32, 'epochs': 5}. Best is trial 1 with value: 0.7793840351979887.\n",
      "2025-11-19 11:38:40.326056: E external/local_xla/xla/service/slow_operation_alarm.cc:73] \n",
      "********************************\n",
      "[Compiling module a_inference_one_step_on_data_75107__.1173] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "2025-11-19 11:35:28.889996: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 420.399948ms\n",
      "\n",
      "********************************\n",
      "[Compiling module a_inference_one_step_on_data_75107__.1173] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "********************************\n",
      "[I 2025-11-19 11:35:30,450] Trial 2 finished with value: 0.7589229805886036 and parameters: {'dense_units': 128, 'dropout_rate': 0.22216398100069698, 'learning_rate': 0.00012379818549968462, 'batch_size': 128, 'epochs': 3}. Best is trial 1 with value: 0.7793840351979887.\n",
      "2025-11-19 11:35:31.321381: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-19 11:35:31.825013: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_993', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "[I 2025-11-19 11:35:35,541] Trial 3 finished with value: 0.7909319899244333 and parameters: {'dense_units': 128, 'dropout_rate': 0.3182215029817129, 'learning_rate': 0.003098714884149096, 'batch_size': 64, 'epochs': 5}. Best is trial 3 with value: 0.7909319899244333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val F1: 0.7909319899244333\n",
      "Best params: {'dense_units': 128, 'dropout_rate': 0.3182215029817129, 'learning_rate': 0.003098714884149096, 'batch_size': 64, 'epochs': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/19 11:35:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.79\n",
      "Test precision: 0.7857142857142857\n",
      "Test recall: 0.7975\n",
      "Test F1: 0.7915632754342432\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n"
     ]
    }
   ],
   "source": [
    "# USE-based classifier with Optuna + MLflow\n",
    "# (replacing the BiLSTM + Word2Vec part from Model 2)\n",
    "\n",
    "def build_use_model(params, input_dim=USE_EMBED_DIM) -> tf.keras.Model:\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(input_dim,), dtype=tf.float32, name=\"use_embedding\"),\n",
    "        tf.keras.layers.Dropout(params[\"dropout_rate\"]),\n",
    "        tf.keras.layers.Dense(params[\"dense_units\"], activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(params[\"dropout_rate\"]),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params[\"learning_rate\"])\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "class ValMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_val_vec, y_val):\n",
    "        super().__init__()\n",
    "        self.X_val_vec = X_val_vec\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred_prob = self.model.predict(self.X_val_vec, verbose=0).ravel()\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "        acc = accuracy_score(self.y_val, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            self.y_val,\n",
    "            y_pred,\n",
    "            average=\"binary\",\n",
    "            zero_division=0,\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"val_accuracy_trace\", acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_precision_trace\", prec, step=epoch)\n",
    "        mlflow.log_metric(\"val_recall_trace\", rec, step=epoch)\n",
    "        mlflow.log_metric(\"val_f1_trace\", f1, step=epoch)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    params = {\n",
    "        \"dense_units\": trial.suggest_int(\"dense_units\", 64, 256, step=64),\n",
    "        \"dropout_rate\": trial.suggest_float(\"dropout_rate\", 0.1, 0.5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 3, 8),\n",
    "    }\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"use_trial_{trial.number}\", nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        train_ds = (\n",
    "            tf.data.Dataset.from_tensor_slices((X_train_vec, y_train))\n",
    "            .shuffle(10000)\n",
    "            .batch(params[\"batch_size\"])\n",
    "        )\n",
    "\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((X_val_vec, y_val)).batch(\n",
    "            params[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "        model = build_use_model(params)\n",
    "\n",
    "        model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=params[\"epochs\"],\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        y_val_pred_prob = model.predict(X_val_vec, verbose=0).ravel()\n",
    "        y_val_pred = (y_val_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "        acc = accuracy_score(y_val, y_val_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_val,\n",
    "            y_val_pred,\n",
    "            average=\"binary\",\n",
    "            zero_division=0,\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"val_accuracy\", acc)\n",
    "        mlflow.log_metric(\"val_precision\", prec)\n",
    "        mlflow.log_metric(\"val_recall\", rec)\n",
    "        mlflow.log_metric(\"val_f1\", f1)\n",
    "\n",
    "        return f1\n",
    "\n",
    "\n",
    "# Parent run: Optuna + final training + evaluation, mirroring Model 2 logic\n",
    "with mlflow.start_run(run_name=\"optuna_use_parent\"):\n",
    "    # Hyperparameter search\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=4)\n",
    "\n",
    "    # Optional Optuna visualizations\n",
    "    try:\n",
    "        import plotly  # noqa: F401\n",
    "\n",
    "        fig1 = optuna.visualization.plot_optimization_history(study)\n",
    "        mlflow.log_figure(fig1, \"optimization_history_use.html\")\n",
    "\n",
    "        fig2 = optuna.visualization.plot_param_importances(study)\n",
    "        mlflow.log_figure(fig2, \"param_importance_use.html\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Plotly non installé — visualisations sautées.\")\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_params = best_trial.params\n",
    "\n",
    "    print(\"Best val F1:\", best_trial.value)\n",
    "    print(\"Best params:\", best_params)\n",
    "\n",
    "    mlflow.log_metric(\"best_val_f1\", best_trial.value)\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
    "\n",
    "    # Re-train on full train + val\n",
    "    X_train_full_vec = np.concatenate([X_train_vec, X_val_vec], axis=0)\n",
    "    y_train_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "    model_best = build_use_model(best_params)\n",
    "\n",
    "    cb = ValMetricsCallback(X_val_vec=X_test_vec, y_val=y_test)\n",
    "\n",
    "    train_full_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train_full_vec, y_train_full))\n",
    "        .shuffle(10000)\n",
    "        .batch(best_params[\"batch_size\"])\n",
    "    )\n",
    "\n",
    "    model_best.fit(\n",
    "        train_full_ds,\n",
    "        epochs=best_params[\"epochs\"],\n",
    "        callbacks=[cb],\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Final test evaluation\n",
    "    y_test_pred_prob = model_best.predict(\n",
    "        X_test_vec,\n",
    "        verbose=0,\n",
    "    ).ravel()\n",
    "    y_test_pred = (y_test_pred_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec, test_rec, test_f1, _ = precision_recall_fscore_support(\n",
    "        y_test,\n",
    "        y_test_pred,\n",
    "        average=\"binary\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    print(\"Test accuracy:\", test_acc)\n",
    "    print(\"Test precision:\", test_prec)\n",
    "    print(\"Test recall:\", test_rec)\n",
    "    print(\"Test F1:\", test_f1)\n",
    "\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    mlflow.log_metric(\"test_precision\", test_prec)\n",
    "    mlflow.log_metric(\"test_recall\", test_rec)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    fig_cm, ax_cm = plt.subplots(figsize=(4, 4))\n",
    "    im = ax_cm.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    ax_cm.figure.colorbar(im, ax=ax_cm)\n",
    "    ax_cm.set(\n",
    "        xticks=[0, 1],\n",
    "        yticks=[0, 1],\n",
    "        xticklabels=[\"Prédit négatif\", \"Prédit positif\"],\n",
    "        yticklabels=[\"Réel négatif\", \"Réel positif\"],\n",
    "        ylabel=\"Réel\",\n",
    "        xlabel=\"Prédit\",\n",
    "        title=\"Matrice de confusion - USE\",\n",
    "    )\n",
    "    labels_cm = [[\"TN\", \"FP\"], [\"FN\", \"TP\"]]\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax_cm.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{labels_cm[i][j]} = {cm[i, j]}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"black\",\n",
    "            )\n",
    "    fig_cm.tight_layout()\n",
    "    mlflow.log_figure(fig_cm, \"confusion_matrix_use.png\")\n",
    "    plt.close(fig_cm)\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_pred_prob)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    fig_auc, ax_auc = plt.subplots(figsize=(4, 4))\n",
    "    ax_auc.plot(fpr, tpr, label=f\"AUC = {auc_score:.3f}\")\n",
    "    ax_auc.plot([0, 1], [0, 1], \"k--\", label=\"Aléatoire\")\n",
    "    ax_auc.set_xlabel(\"False Positive Rate\")\n",
    "    ax_auc.set_ylabel(\"True Positive Rate\")\n",
    "    ax_auc.set_title(\"Courbe ROC - USE\")\n",
    "    ax_auc.legend(loc=\"lower right\")\n",
    "    fig_auc.tight_layout()\n",
    "    mlflow.log_figure(fig_auc, \"roc_curve_use.png\")\n",
    "    plt.close(fig_auc)\n",
    "\n",
    "    mlflow.log_metric(\"test_auc\", auc_score)\n",
    "\n",
    "    # Log model artifact\n",
    "    input_example = X_test_vec[:1]\n",
    "    mlflow.tensorflow.log_model(\n",
    "        model=model_best,\n",
    "        artifact_path=\"best_model_use\",\n",
    "        input_example=input_example,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "714312b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m65,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">197,381</span> (771.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m197,381\u001b[0m (771.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">65,793</span> (257.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m65,793\u001b[0m (257.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">131,588</span> (514.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m131,588\u001b[0m (514.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_best.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e0ab13",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.p7venv)",
   "language": "python",
   "name": "p7venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
