{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8c280806",
      "metadata": {},
      "source": [
        "# Notebook 06 - Inference checks and RAM usage\n",
        "This notebook runs one inference per saved model in Mes_notebooks and reports peak RSS.\n",
        "For each model we run the inference + the needing preprocessing steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d01c6d3",
      "metadata": {},
      "source": [
        "Peak_RSS:\n",
        "\n",
        "The peak_RSS is the maximum amount of random access memory (RAM) that a program has used at any given time during its execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4669cb48",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "import threading\n",
        "import time\n",
        "\n",
        "import psutil\n",
        "\n",
        "NOTEBOOKS_DIR_OVERRIDE = os.environ.get(\"NOTEBOOKS_DIR\")\n",
        "if NOTEBOOKS_DIR_OVERRIDE:\n",
        "    NOTEBOOKS_DIR = Path(NOTEBOOKS_DIR_OVERRIDE).expanduser().resolve()\n",
        "else:\n",
        "    HERE = Path.cwd().resolve()\n",
        "    if (HERE / \"Mes_notebooks\").is_dir():\n",
        "        NOTEBOOKS_DIR = HERE / \"Mes_notebooks\"\n",
        "    elif HERE.name == \"Mes_notebooks\":\n",
        "        NOTEBOOKS_DIR = HERE\n",
        "    else:\n",
        "        NOTEBOOKS_DIR = None\n",
        "        for parent in [HERE] + list(HERE.parents):\n",
        "            candidate = parent / \"Mes_notebooks\"\n",
        "            if candidate.is_dir():\n",
        "                NOTEBOOKS_DIR = candidate\n",
        "                break\n",
        "        if NOTEBOOKS_DIR is None:\n",
        "            raise FileNotFoundError(\"Mes_notebooks directory not found; set NOTEBOOKS_DIR manually.\")\n",
        "\n",
        "SAMPLE_TEXT = \"I love how friendly this app is!\"\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n",
        "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
        "\n",
        "\n",
        "def human_bytes(num):\n",
        "    step = 1024.0\n",
        "    for unit in [\"B\", \"KiB\", \"MiB\", \"GiB\", \"TiB\"]:\n",
        "        if abs(num) < step:\n",
        "            return f\"{num:.1f} {unit}\"\n",
        "        num /= step\n",
        "    return f\"{num:.1f} PiB\"\n",
        "\n",
        "\n",
        "def run_with_peak_rss(func, interval=0.1):\n",
        "    proc = psutil.Process(os.getpid())\n",
        "    baseline = proc.memory_info().rss\n",
        "    peak = baseline\n",
        "    running = True\n",
        "\n",
        "    def monitor():\n",
        "        nonlocal peak, running\n",
        "        while running:\n",
        "            try:\n",
        "                rss = proc.memory_info().rss\n",
        "            except psutil.Error:\n",
        "                break\n",
        "            peak = max(peak, rss)\n",
        "            time.sleep(interval)\n",
        "\n",
        "    t = threading.Thread(target=monitor, daemon=True)\n",
        "    t.start()\n",
        "    try:\n",
        "        result = func()\n",
        "    finally:\n",
        "        running = False\n",
        "        t.join()\n",
        "    return result, peak, peak - baseline\n",
        "\n",
        "\n",
        "def load_json(path):\n",
        "    if not path.exists():\n",
        "        return {}\n",
        "    return json.loads(path.read_text())\n",
        "\n",
        "def load_tf_input_example(example_path, tf_module):\n",
        "    data = load_json(example_path)\n",
        "    tensors = {}\n",
        "    for key, value in data.items():\n",
        "        try:\n",
        "            tensors[key] = tf_module.convert_to_tensor(value)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return tensors\n",
        "\n",
        "\n",
        "def record_result(results, name, peak_bytes, delta_bytes, details=None, error=None, extra=None):\n",
        "    row = {\n",
        "        \"model\": name,\n",
        "        \"peak_rss\": peak_bytes,\n",
        "        \"peak_rss_human\": human_bytes(peak_bytes) if peak_bytes is not None else None,\n",
        "        \"delta_rss\": delta_bytes,\n",
        "        \"delta_rss_human\": human_bytes(delta_bytes) if delta_bytes is not None else None,\n",
        "        \"details\": details,\n",
        "        \"error\": str(error) if error else None,\n",
        "    }\n",
        "    if extra:\n",
        "        row.update(extra)\n",
        "    results.append(row)\n",
        "\n",
        "\n",
        "results = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13f1d35b",
      "metadata": {},
      "source": [
        "# Model_1_simple: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc598fc2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logreg_model_package_minmaxscaler: pred=[1] preprocess_peak=224.0 MiB infer_peak=225.5 MiB\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import PorterStemmer\n",
        "except Exception:\n",
        "    nltk = None\n",
        "    stopwords = None\n",
        "    PorterStemmer = None\n",
        "\n",
        "try:\n",
        "    import emoji as _emoji\n",
        "except Exception:\n",
        "    _emoji = None\n",
        "\n",
        "LOGREG_DIR = NOTEBOOKS_DIR / \"Model_1_simple\"\n",
        "logreg_packages = sorted(LOGREG_DIR.glob(\"logreg_model_package_minmaxscaler\"))\n",
        "\n",
        "\n",
        "def _get_tokenizer():\n",
        "    if nltk is None:\n",
        "        return lambda text: re.findall(r\"[A-Za-z]+\", text)\n",
        "    try:\n",
        "        nltk.data.find(\"tokenizers/punkt\")\n",
        "    except LookupError:\n",
        "        return lambda text: re.findall(r\"[A-Za-z]+\", text)\n",
        "    return nltk.word_tokenize\n",
        "\n",
        "\n",
        "def _get_stop_words():\n",
        "    if stopwords is None:\n",
        "        return set()\n",
        "    try:\n",
        "        return set(stopwords.words(\"english\"))\n",
        "    except LookupError:\n",
        "        return set()\n",
        "\n",
        "_TOKENIZE = _get_tokenizer()\n",
        "_STOP_WORDS = _get_stop_words()\n",
        "_STEMMER = PorterStemmer() if PorterStemmer is not None else None\n",
        "\n",
        "EMOTICON_DICT = {\n",
        "    ':)': 'smile', ':-)': 'smile', ':(': 'sad', ':-(': 'sad',\n",
        "    ':d': 'laugh', ':-d': 'laugh', ';)': 'wink', ';-)': 'wink',\n",
        "    ':/': 'annoyed', ':p': 'playful', ':-p': 'playful',\n",
        "    \":'(\": 'cry', 'xd': 'laugh', 't_t': 'cry'\n",
        "}\n",
        "\n",
        "CONTRACTIONS = {\n",
        "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\", \"isn't\": \"is not\", \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\", \"shouldn't\": \"should not\", \"that's\": \"that is\",\n",
        "    \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\",\n",
        "    \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\", \"what's\": \"what is\", \"where's\": \"where is\",\n",
        "    \"who's\": \"who is\", \"won't\": \"will not\", \"wouldn't\": \"would not\",\n",
        "    \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\", \"gonna\": \"going to\", \"wanna\": \"want to\"\n",
        "}\n",
        "\n",
        "_CONTRACTION_RE = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, CONTRACTIONS.keys())) + r\")\\b\")\n",
        "\n",
        "\n",
        "def _expand_contractions(text):\n",
        "    return _CONTRACTION_RE.sub(lambda m: CONTRACTIONS[m.group(0)], text)\n",
        "\n",
        "\n",
        "def _convert_emojis(text):\n",
        "    if _emoji is None:\n",
        "        return text\n",
        "    return _emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "\n",
        "\n",
        "NEGATIVE_FORMS = [\n",
        "    r\"\\bnot\\b\", r\"\\bno\\b\", r\"\\bnever\\b\", r\"\\bnone\\b\", r\"\\bnothing\\b\", r\"\\bnowhere\\b\",\n",
        "    r\"\\bneither\\b\", r\"\\bnor\\b\", r\"\\bcannot\\b\", r\"\\bwithout\\b\",\n",
        "    r\"\\bdon't\\b\", r\"\\bdoesn't\\b\", r\"\\bdidn't\\b\", r\"\\bcan't\\b\", r\"\\bcouldn't\\b\",\n",
        "    r\"\\bwon't\\b\", r\"\\bwouldn't\\b\", r\"\\bshouldn't\\b\", r\"\\bisn't\\b\", r\"\\baren't\\b\",\n",
        "    r\"\\bwasn't\\b\", r\"\\bweren't\\b\", r\"\\bhasn't\\b\", r\"\\bhaven't\\b\", r\"\\bhadn't\\b\",\n",
        "    r\"\\bain't\\b\", r\"n't\\b\"\n",
        "]\n",
        "\n",
        "_NEG_FORM_PATTERN = re.compile(\"|\".join(NEGATIVE_FORMS), flags=re.IGNORECASE)\n",
        "_HASHTAG_PATTERN = re.compile(r\"#\\w+\")\n",
        "_URL_PATTERN = re.compile(r\"http[s]?://\\S+|www\\.\\S+\")\n",
        "_MENTION_PATTERN = re.compile(r\"@\\w+\")\n",
        "_QUESTION_PATTERN = re.compile(r\"\\?\")\n",
        "_EXCLAMATION_PATTERN = re.compile(r\"!\")\n",
        "_ELLIPSIS_PATTERN = re.compile(r\"\\.\\.\\.\")\n",
        "_UPPERCASE_PATTERN = re.compile(r\"\\b(?:[A-Z]{2,}(?:\\s+[A-Z]{2,})*)\\b\")\n",
        "\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "    text = str(text).lower()\n",
        "    text = _expand_contractions(text)\n",
        "\n",
        "    for emoticon, meaning in EMOTICON_DICT.items():\n",
        "        text = text.replace(emoticon, f\" {meaning} \")\n",
        "\n",
        "    text = _convert_emojis(text)\n",
        "\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    text = re.sub(r'\\bhttps?\\b', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[\\?\\!\\.]', '', text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    tokens = _TOKENIZE(text)\n",
        "    tokens = [word for word in tokens if word not in _STOP_WORDS and len(word) > 1]\n",
        "    if _STEMMER is not None:\n",
        "        tokens = [_STEMMER.stem(word) for word in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def extract_numeric_features(text):\n",
        "    raw = str(text)\n",
        "    return {\n",
        "        \"text_length\": len(raw),\n",
        "        \"word_count\": len(raw.split()),\n",
        "        \"neg_form_count\": len(_NEG_FORM_PATTERN.findall(raw)),\n",
        "        \"hashtags_count\": len(_HASHTAG_PATTERN.findall(raw)),\n",
        "        \"urls_count\": len(_URL_PATTERN.findall(raw)),\n",
        "        \"mentions_count\": len(_MENTION_PATTERN.findall(raw)),\n",
        "        \"question_count\": len(_QUESTION_PATTERN.findall(raw)),\n",
        "        \"exclamation_count\": len(_EXCLAMATION_PATTERN.findall(raw)),\n",
        "        \"ellipsis_count\": len(_ELLIPSIS_PATTERN.findall(raw)),\n",
        "        \"uppercase_count\": len(_UPPERCASE_PATTERN.findall(raw)),\n",
        "    }\n",
        "\n",
        "\n",
        "def build_feature_vector(tfidf, feature_columns, sample_text, scaler=None):\n",
        "    raw_text = str(sample_text)\n",
        "    numeric = extract_numeric_features(raw_text)\n",
        "    processed = preprocess_tweet(raw_text)\n",
        "\n",
        "    tfidf_vec = tfidf.transform([processed]).toarray().ravel()\n",
        "    tfidf_names = [f\"tfidf_{w}\" for w in tfidf.get_feature_names_out()]\n",
        "\n",
        "    col_index = {name: idx for idx, name in enumerate(feature_columns)}\n",
        "    X = np.zeros((1, len(feature_columns)), dtype=float)\n",
        "\n",
        "    for key, value in numeric.items():\n",
        "        idx = col_index.get(key)\n",
        "        if idx is not None:\n",
        "            X[0, idx] = value\n",
        "\n",
        "    for name, value in zip(tfidf_names, tfidf_vec):\n",
        "        idx = col_index.get(name)\n",
        "        if idx is not None:\n",
        "            X[0, idx] = value\n",
        "\n",
        "    if scaler is not None:\n",
        "        X_input = X\n",
        "        if pd is not None and hasattr(scaler, \"feature_names_in_\"):\n",
        "            X_input = pd.DataFrame(X, columns=feature_columns)\n",
        "        X = scaler.transform(X_input)\n",
        "\n",
        "    return X, {\n",
        "        \"preprocessed_text\": processed,\n",
        "        \"numeric_features\": numeric,\n",
        "    }\n",
        "\n",
        "\n",
        "def infer_logreg(package_dir, sample_text=SAMPLE_TEXT):\n",
        "    tfidf = joblib.load(package_dir / \"tfidf.joblib\")\n",
        "    model = joblib.load(package_dir / \"model.joblib\")\n",
        "\n",
        "    feature_columns_path = package_dir / \"feature_columns.json\"\n",
        "    if feature_columns_path.exists():\n",
        "        feature_columns = json.loads(feature_columns_path.read_text())\n",
        "    else:\n",
        "        feature_columns = [f\"tfidf_{w}\" for w in tfidf.get_feature_names_out()]\n",
        "\n",
        "    scaler_path = package_dir / \"scaler.joblib\"\n",
        "    scaler = joblib.load(scaler_path) if scaler_path.exists() else None\n",
        "    if scaler is not None and hasattr(scaler, \"feature_names_in_\"):\n",
        "        feature_columns = list(scaler.feature_names_in_)\n",
        "\n",
        "    def run_preprocess():\n",
        "        return build_feature_vector(tfidf, feature_columns, sample_text, scaler=scaler)\n",
        "\n",
        "    (X, prep_info), pre_peak, pre_delta = run_with_peak_rss(run_preprocess)\n",
        "\n",
        "    def run_infer():\n",
        "        X_input = X\n",
        "        if pd is not None and hasattr(model, \"feature_names_in_\"):\n",
        "            X_input = pd.DataFrame(X, columns=feature_columns)\n",
        "        pred = model.predict(X_input)\n",
        "        proba = model.predict_proba(X_input) if hasattr(model, \"predict_proba\") else None\n",
        "        return pred, proba\n",
        "\n",
        "    (pred, proba), inf_peak, inf_delta = run_with_peak_rss(run_infer)\n",
        "\n",
        "    return {\n",
        "        \"pred\": pred.tolist(),\n",
        "        \"proba\": proba.tolist() if proba is not None else None,\n",
        "        \"preprocessed_text\": prep_info[\"preprocessed_text\"],\n",
        "        \"numeric_features\": prep_info[\"numeric_features\"],\n",
        "        \"feature_count\": len(feature_columns),\n",
        "    }, pre_peak, pre_delta, inf_peak, inf_delta\n",
        "\n",
        "\n",
        "if not logreg_packages:\n",
        "    print(\"No logreg model packages found.\")\n",
        "\n",
        "for package_dir in logreg_packages:\n",
        "    try:\n",
        "        output, pre_peak, pre_delta, inf_peak, inf_delta = infer_logreg(package_dir)\n",
        "        extra = {\n",
        "            \"preprocess_peak_rss\": pre_peak,\n",
        "            \"preprocess_peak_rss_human\": human_bytes(pre_peak),\n",
        "            \"preprocess_peak_rss\": pre_delta,\n",
        "            \"preprocess_peak_rss_human\": human_bytes(pre_peak),\n",
        "            \"inference_peak_rss\": inf_peak,\n",
        "            \"inference_peak_rss_human\": human_bytes(inf_peak),\n",
        "            \"inference_delta_rss\": inf_delta,\n",
        "            \"inference_delta_rss_human\": human_bytes(inf_peak),\n",
        "        }\n",
        "        record_result(\n",
        "            results,\n",
        "            f\"logreg:{package_dir.name}\",\n",
        "            inf_peak,\n",
        "            inf_delta,\n",
        "            details=output,\n",
        "            extra=extra,\n",
        "        )\n",
        "        print(\n",
        "            f\"{package_dir.name}: pred={output['pred']} preprocess_peak={human_bytes(pre_peak)} \"\n",
        "            f\"infer_peak={human_bytes(inf_peak)}\"\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        record_result(results, f\"logreg:{package_dir.name}\", None, None, error=exc)\n",
        "        print(f\"{package_dir.name}: error={exc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92cb4724",
      "metadata": {},
      "source": [
        "# Model_2_advanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6a1a0173",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 10:24:30.682437: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-05 10:24:30.713813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-01-05 10:24:33.001030: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2026-01-05 10:24:33.344905: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BOOL only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bilstm_w2v_model_package: backend=keras preprocess_peak=731.2 MiB infer_peak=998.2 MiB\n",
            "bilstm_glove_model_package: backend=keras preprocess_peak=929.5 MiB infer_peak=930.0 MiB\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from typing import List, Tuple\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "\n",
        "try:\n",
        "    from emoji import demojize\n",
        "    _HAS_EMOJI = True\n",
        "except Exception:\n",
        "    _HAS_EMOJI = False\n",
        "\n",
        "BILSTM_DIR = NOTEBOOKS_DIR / \"Model_2_advanced\"\n",
        "bilstm_packages = [\n",
        "    BILSTM_DIR / \"bilstm_w2v_model_package\",\n",
        "    BILSTM_DIR / \"bilstm_glove_model_package\",\n",
        "]\n",
        "\n",
        "# ---------- light, meaning-preserving normalization (from notebook_02) ----------\n",
        "_EMOTICONS = {\n",
        "    r\":-\\)|:\\)|=\\)|:\\]\": \"smile\",\n",
        "    r\":-D|:D|=D\": \"laugh\",\n",
        "    r\":-\\(|:\\(|=\\(|:\\[\": \"sad\",\n",
        "    r\":'\\(|:'-\\(\": \"cry\",\n",
        "    r\";-\\)|;\\)\": \"wink\",\n",
        "    r\":-P|:P\": \"playful\",\n",
        "    r\":/|:-/\": \"skeptical\",\n",
        "    r\":\\*\": \"kiss\",\n",
        "    r\">:\\(|>:-\\(\": \"angry\",\n",
        "    r\"XD|xD\": \"laugh\",\n",
        "}\n",
        "\n",
        "_EMOTICON_REGEXES = [(re.compile(p), w) for p, w in _EMOTICONS.items()]\n",
        "\n",
        "_URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
        "_USER_RE = re.compile(r\"@\\w+\")\n",
        "_NUM_RE = re.compile(r\"\\b\\d+\\b\")\n",
        "_HASHTAG_RE = re.compile(r\"#(\\w+)\")\n",
        "_REPEAT_RE = re.compile(r\"(.)\\1{3,}\")\n",
        "\n",
        "\n",
        "def _emoticons_to_words(text: str) -> str:\n",
        "    for rgx, word in _EMOTICON_REGEXES:\n",
        "        text = rgx.sub(f\" {word} \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def _emojis_to_words(text: str) -> str:\n",
        "    if not _HAS_EMOJI:\n",
        "        return text\n",
        "    text = demojize(text, language=\"en\")\n",
        "    text = re.sub(r\":([a-zA-Z0-9_]+):\", lambda m: \" \" + m.group(1).replace(\"_\", \" \") + \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize_tweet(t: str) -> str:\n",
        "    t = t.strip().lower()\n",
        "    t = _URL_RE.sub(\" <URL> \", t)\n",
        "    t = _USER_RE.sub(\" <USER> \", t)\n",
        "    t = _NUM_RE.sub(\" <NUM> \", t)\n",
        "    t = _emoticons_to_words(t)\n",
        "    t = _emojis_to_words(t)\n",
        "    t = _HASHTAG_RE.sub(lambda m: f\" #{m.group(1)} {m.group(1)} \", t)\n",
        "    t = _REPEAT_RE.sub(r\"\\1\\1\\1\", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "\n",
        "def preprocess_test(test_texts: List[str], tokenizer, max_len: int) -> Tuple[List[List[int]], List[List[int]]]:\n",
        "    norm = [normalize_tweet(t) for t in test_texts]\n",
        "    seqs = tokenizer.texts_to_sequences(norm)\n",
        "    padded = pad_sequences(seqs, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "    masks = (padded != 0).astype(\"int32\").tolist()\n",
        "    return padded, masks\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    if not tokenizer_path.exists():\n",
        "        raise FileNotFoundError(f\"Missing tokenizer.json in {tokenizer_path.parent}\")\n",
        "    return tokenizer_from_json(tokenizer_path.read_text())\n",
        "\n",
        "\n",
        "def get_max_len(example_path, default=50):\n",
        "    example = load_json(example_path)\n",
        "    token_ids = example.get(\"token_ids\")\n",
        "    if isinstance(token_ids, list) and token_ids and isinstance(token_ids[0], list):\n",
        "        return len(token_ids[0])\n",
        "    return default\n",
        "\n",
        "\n",
        "def prepare_bilstm_inputs(package_dir, texts):\n",
        "    tokenizer_path = package_dir / \"tokenizer.json\"\n",
        "    example_path = package_dir / \"input_example.json\"\n",
        "    tokenizer = load_tokenizer(tokenizer_path)\n",
        "    max_len = get_max_len(example_path)\n",
        "    tokens, masks = preprocess_test(texts, tokenizer, max_len)\n",
        "    return tokens, masks, {\n",
        "        \"max_len\": max_len,\n",
        "    }\n",
        "\n",
        "\n",
        "def infer_bilstm(package_dir, sample_text=SAMPLE_TEXT, prefer_keras=True):\n",
        "    saved_model_dir = package_dir / \"saved_model\"\n",
        "    keras_path = package_dir / \"keras_model\" / \"model.keras\"\n",
        "\n",
        "    def run_preprocess():\n",
        "        return prepare_bilstm_inputs(package_dir, [sample_text])\n",
        "\n",
        "    (tokens, masks, prep_info), pre_peak, pre_delta = run_with_peak_rss(run_preprocess)\n",
        "\n",
        "    def run_infer():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        try:\n",
        "            tf.config.set_visible_devices([], \"GPU\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        if prefer_keras and keras_path.exists():\n",
        "            model = tf.keras.models.load_model(keras_path)\n",
        "            x = tf.convert_to_tensor(tokens, dtype=tf.int32)\n",
        "            out = model(x)\n",
        "            shape = getattr(out, \"shape\", None)\n",
        "            if hasattr(shape, \"as_list\"):\n",
        "                shape = shape.as_list()\n",
        "            elif shape is not None:\n",
        "                shape = list(shape)\n",
        "            return {\n",
        "                \"backend\": \"keras\",\n",
        "                \"output_shape\": shape,\n",
        "                \"max_len\": prep_info[\"max_len\"],\n",
        "            }\n",
        "\n",
        "        if saved_model_dir.exists():\n",
        "            model = tf.saved_model.load(saved_model_dir)\n",
        "            infer = model.signatures.get(\"serving_default\")\n",
        "            if infer is None:\n",
        "                raise RuntimeError(\"No serving_default signature.\")\n",
        "\n",
        "            expected = infer.structured_input_signature[1]\n",
        "            feed = {}\n",
        "            for name, spec in expected.items():\n",
        "                if name in {\"token_ids\", \"input_ids\", \"input\", \"input_1\", \"keras_tensor_8\"}:\n",
        "                    feed[name] = tf.convert_to_tensor(tokens, dtype=spec.dtype or tf.int32)\n",
        "                elif \"mask\" in name and masks is not None:\n",
        "                    feed[name] = tf.convert_to_tensor(masks, dtype=spec.dtype or tf.int32)\n",
        "\n",
        "            if not feed and len(expected) == 1:\n",
        "                name, spec = next(iter(expected.items()))\n",
        "                feed[name] = tf.convert_to_tensor(tokens, dtype=spec.dtype or tf.int32)\n",
        "\n",
        "            if not feed:\n",
        "                for name, spec in expected.items():\n",
        "                    shape = [1 if dim is None else dim for dim in spec.shape]\n",
        "                    feed[name] = tf.zeros(shape, dtype=spec.dtype)\n",
        "\n",
        "            out = infer(**feed)\n",
        "            return {\n",
        "                \"backend\": \"saved_model\",\n",
        "                \"output_keys\": list(out.keys()),\n",
        "                \"max_len\": prep_info[\"max_len\"],\n",
        "            }\n",
        "\n",
        "        raise RuntimeError(\"No keras or saved_model found.\")\n",
        "\n",
        "    output, inf_peak, inf_delta = run_with_peak_rss(run_infer)\n",
        "    return output, pre_peak, pre_delta, inf_peak, inf_delta\n",
        "\n",
        "\n",
        "for package_dir in bilstm_packages:\n",
        "    if not package_dir.exists():\n",
        "        print(f\"Missing package: {package_dir}\")\n",
        "        continue\n",
        "    try:\n",
        "        output, pre_peak, pre_delta, inf_peak, inf_delta = infer_bilstm(package_dir, prefer_keras=True)\n",
        "        extra = {\n",
        "            \"preprocess_peak_rss\": pre_peak,\n",
        "            \"preprocess_peak_rss_human\": human_bytes(pre_peak),\n",
        "            \"preprocess_delta_rss\": pre_delta,\n",
        "            \"preprocess_delta_rss_human\": human_bytes(pre_delta),\n",
        "            \"inference_peak_rss\": inf_peak,\n",
        "            \"inference_peak_rss_human\": human_bytes(inf_peak),\n",
        "            \"inference_delta_rss\": inf_delta,\n",
        "            \"inference_delta_rss_human\": human_bytes(inf_delta),\n",
        "        }\n",
        "        record_result(results, f\"bilstm:{package_dir.name}\", inf_peak, inf_delta, details=output, extra=extra)\n",
        "        print(\n",
        "            f\"{package_dir.name}: backend={output['backend']} preprocess_peak={human_bytes(pre_peak)} \"\n",
        "            f\"infer_peak={human_bytes(inf_peak)}\"\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        record_result(results, f\"bilstm:{package_dir.name}\", None, None, error=exc)\n",
        "        print(f\"{package_dir.name}: error={exc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4d2239",
      "metadata": {},
      "source": [
        "# Model_3_USE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "509024f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 10:22:25.064971: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-05 10:22:25.610568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "use_model_package: preprocess_peak=64.9 MiB infer_peak=742.9 MiB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 10:22:29.707779: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "try:\n",
        "    from emoji import demojize\n",
        "    _HAS_EMOJI = True\n",
        "except Exception:\n",
        "    _HAS_EMOJI = False\n",
        "\n",
        "USE_DIR = NOTEBOOKS_DIR / \"Model_3_USE\" / \"use_model_package\"\n",
        "\n",
        "# ---------- cleaning text (from notebook_03) ----------\n",
        "_EMOTICONS = {\n",
        "    r\":-\\)|:\\)|=\\)|:\\]\": \"smile\",\n",
        "    r\":-D|:D|=D\": \"laugh\",\n",
        "    r\":-\\(|:\\(|=\\(|:\\[\": \"sad\",\n",
        "    r\":'\\(|:'-\\(\": \"cry\",\n",
        "    r\";-\\)|;\\)\": \"wink\",\n",
        "    r\":-P|:P\": \"playful\",\n",
        "    r\":/|:-/\": \"skeptical\",\n",
        "    r\":\\*\": \"kiss\",\n",
        "    r\">:\\(|>:-\\(\": \"angry\",\n",
        "    r\"XD|xD\": \"laugh\",\n",
        "}\n",
        "\n",
        "_EMOTICON_REGEXES = [(re.compile(p), w) for p, w in _EMOTICONS.items()]\n",
        "\n",
        "_URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
        "_USER_RE = re.compile(r\"@\\w+\")\n",
        "_NUM_RE = re.compile(r\"\\b\\d+\\b\")\n",
        "_HASHTAG_RE = re.compile(r\"#(\\w+)\")\n",
        "_REPEAT_RE = re.compile(r\"(.)\\1{3,}\")\n",
        "\n",
        "\n",
        "def _emoticons_to_words(text: str) -> str:\n",
        "    for rgx, word in _EMOTICON_REGEXES:\n",
        "        text = rgx.sub(f\" {word} \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def _emojis_to_words(text: str) -> str:\n",
        "    if not _HAS_EMOJI:\n",
        "        return text\n",
        "    text = demojize(text, language=\"en\")\n",
        "    text = re.sub(r\":([a-zA-Z0-9_]+):\", lambda m: \" \" + m.group(1).replace(\"_\", \" \") + \" \", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize_tweet(t: str) -> str:\n",
        "    t = t.strip().lower()\n",
        "    t = _URL_RE.sub(\" <URL> \", t)\n",
        "    t = _USER_RE.sub(\" <USER> \", t)\n",
        "    t = _NUM_RE.sub(\" <NUM> \", t)\n",
        "    t = _emoticons_to_words(t)\n",
        "    t = _emojis_to_words(t)\n",
        "    t = _HASHTAG_RE.sub(lambda m: f\" #{m.group(1)} {m.group(1)} \", t)\n",
        "    t = _REPEAT_RE.sub(r\"\\1\\1\\1\", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "\n",
        "def load_tf_input_example(example_path, tf_module):\n",
        "    data = load_json(example_path)\n",
        "    tensors = {}\n",
        "    for key, value in data.items():\n",
        "        try:\n",
        "            tensors[key] = tf_module.convert_to_tensor(value)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return tensors\n",
        "\n",
        "\n",
        "def infer_use(package_dir):\n",
        "    saved_model_dir = package_dir / \"saved_model\"\n",
        "    example_path = package_dir / \"input_example.json\"\n",
        "\n",
        "    def run_preprocess():\n",
        "        raw = SAMPLE_TEXT\n",
        "        normalized = normalize_tweet(raw)\n",
        "        return normalized\n",
        "\n",
        "    normalized_text, pre_peak, pre_delta = run_with_peak_rss(run_preprocess)\n",
        "\n",
        "    def run_infer():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        try:\n",
        "            tf.config.set_visible_devices([], \"GPU\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        model = tf.saved_model.load(saved_model_dir)\n",
        "        infer = model.signatures.get(\"serving_default\")\n",
        "        if infer is None:\n",
        "            raise RuntimeError(\"No serving_default signature.\")\n",
        "\n",
        "        example_inputs = load_tf_input_example(example_path, tf)\n",
        "        expected = infer.structured_input_signature[1]\n",
        "        feed = {}\n",
        "        for name, spec in expected.items():\n",
        "            if name in example_inputs:\n",
        "                feed[name] = tf.convert_to_tensor(example_inputs[name], dtype=spec.dtype)\n",
        "            elif name == \"input_1\":\n",
        "                feed[name] = tf.convert_to_tensor([normalized_text], dtype=spec.dtype or tf.string)\n",
        "\n",
        "        if not feed:\n",
        "            for name, spec in expected.items():\n",
        "                shape = [1 if dim is None else dim for dim in spec.shape]\n",
        "                if spec.dtype == tf.string:\n",
        "                    feed[name] = tf.convert_to_tensor([normalized_text], dtype=tf.string)\n",
        "                else:\n",
        "                    feed[name] = tf.zeros(shape, dtype=spec.dtype)\n",
        "\n",
        "        out = infer(**feed)\n",
        "        return {\n",
        "            \"output_keys\": list(out.keys()),\n",
        "        }\n",
        "\n",
        "    output, inf_peak, inf_delta = run_with_peak_rss(run_infer)\n",
        "    return output, pre_peak, pre_delta, inf_peak, inf_delta\n",
        "\n",
        "\n",
        "if not USE_DIR.exists():\n",
        "    print(f\"Missing package: {USE_DIR}\")\n",
        "else:\n",
        "    try:\n",
        "        output, pre_peak, pre_delta, inf_peak, inf_delta = infer_use(USE_DIR)\n",
        "        extra = {\n",
        "            \"preprocess_peak_rss\": pre_peak,\n",
        "            \"preprocess_peak_rss_human\": human_bytes(pre_peak),\n",
        "            \"preprocess_delta_rss\": pre_delta,\n",
        "            \"preprocess_delta_rss_human\": human_bytes(pre_delta),\n",
        "            \"inference_peak_rss\": inf_peak,\n",
        "            \"inference_peak_rss_human\": human_bytes(inf_peak),\n",
        "            \"inference_delta_rss\": inf_delta,\n",
        "            \"inference_delta_rss_human\": human_bytes(inf_delta),\n",
        "        }\n",
        "        record_result(results, \"use:use_model_package\", inf_peak, inf_delta, details=output, extra=extra)\n",
        "        print(f\"use_model_package: preprocess_peak={human_bytes(pre_peak)} infer_peak={human_bytes(inf_peak)}\")\n",
        "    except Exception as exc:\n",
        "        record_result(results, \"use:use_model_package\", None, None, error=exc)\n",
        "        print(f\"use_model_package: error={exc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a28b713",
      "metadata": {},
      "source": [
        "# Model_4_DISTILBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2d4a5690",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 10:16:22.744611: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-05 10:16:22.771271: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "distilbert hf_model: peak_rss=1.2 GiB\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "DISTIL_DIR = NOTEBOOKS_DIR / \"Model_4_DISTILBERT\" / \"distilbert_model_package\"\n",
        "TOKENIZER_DIR = DISTIL_DIR / \"tokenizer\"\n",
        "HF_MODEL_DIR = DISTIL_DIR / \"hf_model\"\n",
        "\n",
        "def normalize_tweet(t: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", t.strip())\n",
        "\n",
        "def infer_distilbert(sample_text=SAMPLE_TEXT):\n",
        "    def run():\n",
        "        os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"\")\n",
        "        import torch\n",
        "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "        torch.set_grad_enabled(False)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, use_fast=True)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(HF_MODEL_DIR)\n",
        "        model.eval()\n",
        "        clean_text = normalize_tweet(sample_text)\n",
        "        inputs = tokenizer(\n",
        "            clean_text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "        )\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        return {\n",
        "            \"logits\": logits.tolist(),\n",
        "            \"normalized_text\": clean_text,\n",
        "        }\n",
        "\n",
        "    return run_with_peak_rss(run)\n",
        "\n",
        "if not HF_MODEL_DIR.exists():\n",
        "    print(f\"Missing package: {DISTIL_DIR}\")\n",
        "else:\n",
        "    try:\n",
        "        output, peak, delta = infer_distilbert()\n",
        "        record_result(results, \"distilbert:hf_model\", peak, delta, details=output)\n",
        "        print(f\"distilbert hf_model: peak_rss={human_bytes(peak)}\")\n",
        "    except Exception as exc:\n",
        "        record_result(results, \"distilbert:hf_model\", None, None, error=exc)\n",
        "        print(f\"distilbert hf_model: error={exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c98caceb",
      "metadata": {},
      "source": [
        "# Model_4_DISTILBERT_onnx_int8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3dbde146",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kamel/Openclassroom_projets/P7/.p7venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "onnx model-int8-static: peak_rss=665.2 MiB\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "ONNX_DIR = NOTEBOOKS_DIR / \"Model_4_DISTILBERT_quant\"\n",
        "ONNX_PATH = ONNX_DIR / \"model-int8-static.onnx\"\n",
        "TOKENIZER_DIR = NOTEBOOKS_DIR / \"Model_4_DISTILBERT\" / \"distilbert_model_package\" / \"tokenizer\"\n",
        "\n",
        "def normalize_tweet(t: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", t.strip())\n",
        "\n",
        "def infer_onnx(sample_text=SAMPLE_TEXT):\n",
        "    def run():\n",
        "        import onnxruntime as ort\n",
        "        from transformers import AutoTokenizer\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, use_fast=True)\n",
        "        clean_text = normalize_tweet(sample_text)\n",
        "        encoded = tokenizer(\n",
        "            clean_text,\n",
        "            return_tensors=\"np\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "        )\n",
        "        session = ort.InferenceSession(str(ONNX_PATH), providers=[\"CPUExecutionProvider\"])\n",
        "        feed = {}\n",
        "        for input_meta in session.get_inputs():\n",
        "            name = input_meta.name\n",
        "            if name in encoded:\n",
        "                feed[name] = encoded[name]\n",
        "            elif name == \"token_type_ids\":\n",
        "                feed[name] = np.zeros_like(encoded[\"input_ids\"])\n",
        "        outputs = session.run(None, feed)\n",
        "        shapes = [getattr(out, \"shape\", None) for out in outputs]\n",
        "        return {\n",
        "            \"output_shapes\": shapes,\n",
        "            \"normalized_text\": clean_text,\n",
        "        }\n",
        "\n",
        "    return run_with_peak_rss(run)\n",
        "\n",
        "if not ONNX_PATH.exists():\n",
        "    print(f\"Missing package: {ONNX_PATH}\")\n",
        "else:\n",
        "    try:\n",
        "        output, peak, delta = infer_onnx()\n",
        "        record_result(results, \"onnx:model-int8-static\", peak, delta, details=output)\n",
        "        print(f\"onnx model-int8-static: peak_rss={human_bytes(peak)}\")\n",
        "    except Exception as exc:\n",
        "        record_result(results, \"onnx:model-int8-static\", None, None, error=exc)\n",
        "        print(f\"onnx model-int8-static: error={exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa53e293",
      "metadata": {},
      "source": [
        "#  Model_4_DISTILBERT_model-int8-dynamic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f0fb8dcf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "onnx model-int8-dynamic: peak_rss=660.8 MiB\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    ONNX_DIR\n",
        "except NameError:\n",
        "    ONNX_DIR = NOTEBOOKS_DIR / \"Model_4_DISTILBERT_quant\"\n",
        "\n",
        "try:\n",
        "    TOKENIZER_DIR\n",
        "except NameError:\n",
        "    TOKENIZER_DIR = NOTEBOOKS_DIR / \"Model_4_DISTILBERT\" / \"distilbert_model_package\" / \"tokenizer\"\n",
        "\n",
        "try:\n",
        "    normalize_tweet\n",
        "except NameError:\n",
        "    def normalize_tweet(t: str) -> str:\n",
        "        return re.sub(r\"\\s+\", \" \", t.strip())\n",
        "\n",
        "ONNX_DYNAMIC_PATH = ONNX_DIR / \"model-int8-dynamic.onnx\"\n",
        "\n",
        "\n",
        "def infer_onnx_dynamic(sample_text=SAMPLE_TEXT):\n",
        "    def run():\n",
        "        import onnxruntime as ort\n",
        "        from transformers import AutoTokenizer\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, use_fast=True)\n",
        "        clean_text = normalize_tweet(sample_text)\n",
        "        encoded = tokenizer(\n",
        "            clean_text,\n",
        "            return_tensors=\"np\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "        )\n",
        "        session = ort.InferenceSession(str(ONNX_DYNAMIC_PATH), providers=[\"CPUExecutionProvider\"])\n",
        "        feed = {}\n",
        "        for input_meta in session.get_inputs():\n",
        "            name = input_meta.name\n",
        "            if name in encoded:\n",
        "                feed[name] = encoded[name]\n",
        "            elif name == \"token_type_ids\":\n",
        "                feed[name] = np.zeros_like(encoded[\"input_ids\"])\n",
        "        outputs = session.run(None, feed)\n",
        "        shapes = [getattr(out, \"shape\", None) for out in outputs]\n",
        "        return {\n",
        "            \"output_shapes\": shapes,\n",
        "            \"normalized_text\": clean_text,\n",
        "        }\n",
        "\n",
        "    return run_with_peak_rss(run)\n",
        "\n",
        "if not ONNX_DYNAMIC_PATH.exists():\n",
        "    print(f\"Missing package: {ONNX_DYNAMIC_PATH}\")\n",
        "else:\n",
        "    try:\n",
        "        output, peak, delta = infer_onnx_dynamic()\n",
        "        record_result(results, \"onnx:model-int8-dynamic\", peak, delta, details=output)\n",
        "        print(f\"onnx model-int8-dynamic: peak_rss={human_bytes(peak)}\")\n",
        "    except Exception as exc:\n",
        "        record_result(results, \"onnx:model-int8-dynamic\", None, None, error=exc)\n",
        "        print(f\"onnx model-int8-dynamic: error={exc}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (.p7venv)",
      "language": "python",
      "name": "p7venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
