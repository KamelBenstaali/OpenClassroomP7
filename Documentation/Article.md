# Analyse de Sentiments de Tweets gr√¢ce au Deep Learning : Une Approche MLOps

Cet article d√©crit la r√©alisation d'une mission du parcours de la formation [AI Engineer](https://openclassrooms.com/fr/paths/795-ai-engineer) d'[OpenClassrooms](https://openclassrooms.com/fr/), et qui est d'analyser les sentiments de tweets.

# Description de la mission
## Contexte
Dans ce projet, on est ing√©nieur IA chez MIC ( Marketing Intelligence Consulting), entreprise de conseil sp√©cialis√©e en marketing digital.

Notre client, ‚úàÔ∏è Air Paradis (compagnie a√©rienne), souhaite anticiper les bad buzz sur les r√©seaux sociaux, donc celle ci a demand√© √† d√©velopper un produit IA permettant de pr√©dire le sentiment associ√© √† un tweet, pour mieux voir sa r√©putation en ligne.

## Mission

Cr√©er un prototype fonctionnel d'un mod√®le d'analyse de sentiments pour tweets selon trois approches diff√©rentes :

1. **Mod√®le sur mesure simple** : Approche classique (r√©gression logistique) pour une pr√©diction rapide
2. **Mod√®le sur mesure avanc√©** : Utilisation de r√©seaux de neurones profonds avec diff√©rents word embeddings
3. **Mod√®le avanc√© USE** : Exploration du mod√®le USE (v4 qui contient une architecture transformer)
4. **Mod√®le avanc√© BERT** : Exploration de l'apport en performance d'un mod√®le BERT

Cette mission implique √©galement la mise en ≈ìuvre d'une d√©marche MLOps compl√®te :

- Utilisation de MLFlow pour le tracking des exp√©rimentations et le stockage des mod√®les.

- Cr√©ation d'un pipeline de d√©ploiement continu (Git + Github + plateforme Cloud).

- Int√©gration de tests unitaires automatis√©s.

- Mise en place d'un suivi de performance en production via Azure Application Insight.

# üîß Technologies utilis√©es

- Langages : Python
- Biblioth√®ques ML/DL : Scikit-learn, TensorFlow/Keras, Transformers (BERT), Torch
- MLOps :
    - Tracking : MLFlow
    - CI/CD : Git, GiHub Actions
    - Cloud : Azure Web Application (ASP F1)
- Backend : FastAPI
- Frontend : Streamlit
- Monitoring : Azure Application Insight
- Traitement texte : NLTK, Word Embeddings

# Structure du projet
Le d√©p√¥t P7 est organis√© pour s√©parer clairement exploration, code applicatif et documentation, tout en restant lisible pour le MLOps.

Ce d√©p√¥t est disponible ici:
[Github](https://github.com/KamelBenstaali/OpenClassroomP7)

- `Documentation/` : articles et guides (Azure, monitoring, setup).
- `Mes_notebooks/` : notebooks des 4 approches (EDA + simple, BiLSTM, USE, DistilBERT), quantification DistilBERT et notebook 6 (comparatif d‚Äôinf√©rence/RSS).
- `.github/workflows/` : CI/CD GitHub Actions via `azure-webapp.yml` qui installe les d√©pendances FastAPI, lance `pytest tests/test_main.py`, construit une archive et d√©ploie l'API sur Azure App Service (login avec le secret `AZURE_CREDENTIALS`).
- `app/` : code produit avec `FastApi` pour le backend et `FrontEnd` pour l‚Äôinterface utilisateur.
- `tests/` : tests automatis√©s.
- Fichiers racine : `requirements.txt`, `ReadMe.md` pour l‚Äôinstallation/la prise en main, et `.gitignore` pour exclure donn√©es volumineuses et artefacts.

Sch√©ma rapide (r√©pertoires principaux uniquement) :
```
P7
‚îú‚îÄ .github/
‚îÇ  ‚îî‚îÄ workflows/
‚îÇ     ‚îî‚îÄ azure-webapp.yml
‚îú‚îÄ Documentation/
‚îú‚îÄ Mes_notebooks/
‚îÇ  ‚îú‚îÄ Model_1_simple/
‚îÇ  ‚îú‚îÄ Model_2_advanced/
‚îÇ  ‚îú‚îÄ Model_3_USE/
‚îÇ  ‚îú‚îÄ Model_4_DISTILBERT/
‚îÇ  ‚îî‚îÄ Model_4_DISTILBERT_quant/
‚îÇ  ‚îî‚îÄ notebook_06.ipynb
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ FastApi/
‚îÇ  ‚îî‚îÄ FrontEnd/
‚îú‚îÄ tests/
‚îú‚îÄ .gitignore
‚îú‚îÄ ReadMe.md
‚îî‚îÄ requirements.txt
```


# Analyse exploratoire des tweets
Le notebook `Mes_notebooks/Model_1_simple/notebook_01.ipynb` synth√©tise l'EDA r√©alis√©e sur l'ensemble d'entra√Ænement Sentiment140 (1,6 M de tweets, 6 colonnes). Aucun doublon ni valeur manquante n'a √©t√© d√©tect√©, les classes sont parfaitement √©quilibr√©es (800 k n√©gatifs cod√©s `0` et 800 k positifs cod√©s `4`) et la colonne `query` ne contient que `NO_QUERY`, ce qui a conduit √† ne conserver que les variables `sentiment` et `tweet` pour la suite.

Principales analyses men√©es :
- Nuages de mots et top 20 uni-grammes s√©par√©s par polarit√© pour visualiser le vocabulaire dominant dans chaque classe.
- Longueur des tweets : moyenne ‚âà74 caract√®res et ‚âà13 mots, distributions de taille similaires entre sentiments.
- Marques lexicales et ponctuation : 
    - formes n√©gatives (not/never/...) beaucoup plus pr√©sentes dans les tweets n√©gatifs (‚âà69 % des occurrences) ;
    - les points d‚Äôexclamation dominent c√¥t√© positif (‚âà36,6 %), les points d‚Äôinterrogation sont neutres, les ellipses `...` apparaissent davantage dans le n√©gatif (‚âà11,3 %).
- Emotic√¥nes : les tweets positifs contiennent majoritairement des emotic√¥nes positives (‚âà87,4 %) ; m√™me dans les tweets n√©gatifs, les emotic√¥nes positives restent majoritaires (‚âà69 %).
- Hashtags/URLs/@mentions : ces √©l√©ments sont tous plus fr√©quents dans les tweets positifs (mentions ‚âà59,9 %, URLs ‚âà67,2 %, hashtags ‚âà58,3 %), sugg√©rant une communication plus interactive/promotionnelle c√¥t√© positif.
- Usage de majuscules : les expressions enti√®rement en MAJUSCULES sont l√©g√®rement plus pr√©sentes dans les tweets positifs (‚âà52,2 % vs 47,8 %), indice d‚Äôune expressivit√© plus forte.

Ces observations orientent le jeu de features retenu pour le mod√®le classique (comptes de ponctuation, majuscules, hashtags/URLs/mentions, longueur, formes n√©gatives, emotic√¥nes), combin√©s ensuite aux repr√©sentations TF‚ÄëIDF.

# Mod√©lisations

Lors de chaque modelisation, nous avons entrain√© un model sur un √©chantillon de donn√©es puis nous avons √©valu√© le mod√®le sur des m√©triques de base (Accuracy, rappel, F1_score, ...).

Nous avons ajout√© un test sur des tweets d√©finis pour voir les capacit√©s de chaque mod√®le sur des tweets repr√©sentatifs (on note l'accuracy seulement).

Lors de notre benchmark, nous comparerons 3 m√©triques: Accuracy, F1_score, Accuracy sur les tweets personnalis√©s.

## Mod√©lisation Approche Simple
Le notebook `Mes_notebooks/Model_1_simple/notebook_01.ipynb` met en place une version ‚Äúclassique‚Äù du mod√®le pour disposer rapidement d‚Äôun premier benchmark.

- **Nettoyage du texte** : on simplifie l‚Äô√©criture (minuscules, contractions, √©mojis ‚Üí mots), on retire liens, mentions, hashtags et caract√®res parasites, puis on enl√®ve les mots vides pour garder l‚Äôessentiel du message.
- **√âchantillon de travail** : pour aller vite, on prend un extrait √©quilibr√© de 16 000 tweets (moiti√© n√©gatifs, moiti√© positifs) issu du jeu nettoy√©.
- **Ce qu‚Äôon mesure dans chaque tweet** : taille du message, pr√©sence de mots n√©gatifs, hashtags/URLs/mentions, ponctuation marquante (`?`, `!`, `...`), passages en MAJUSCULES.
- **Repr√©sentation du texte** : on transforme les mots en nombres (pond√©ration TF‚ÄëIDF) et on ajoute les mesures pr√©c√©dentes pour enrichir la description.
- **Entra√Ænement et choix du mod√®le** : plusieurs variantes de r√©gression logistique sont test√©es (Avec diff√©rentes standardisations de donn√©es). La meilleure combine les deux familles de signaux (texte + indicateurs simples) et sert de r√©f√©rence pour les approches plus avanc√©es.


## Mod√©lisation Approche avanc√©e
Le notebook `Mes_notebooks/Model_2_advanced/notebook_02.ipynb` pousse plus loin l‚Äôapprentissage en utilisant des r√©seaux BiLSTM et des vecteurs de mots pr√©-entra√Æn√©s.

- **Nettoyage l√©ger** : on simplifie le texte (minuscules, transformation des √©mojis/emotic√¥nes en mots, retrait des liens/mentions/hashtags/nombres) tout en gardant la ponctuation et les mots de liaison pour pr√©server le contexte.
- **Jeu de travail √©quilibr√©** : 8 000 tweets n√©gatifs et 8 000 positifs (comme pour l‚Äôapproche simple), re-√©tiquet√©s en 0/1 puis d√©coup√©s en train/validation/test.
- **Repr√©sentation du texte** : d√©coupage des tweets en s√©quences de mots, limitation du vocabulaire aux 20 000 termes les plus fr√©quents, et longueur standardis√©e (80 tokens) pour alimenter les r√©seaux.
- **Deux variantes de BiLSTM pr√©-entra√Æn√©es** :
  - *Word2Vec GoogleNews* (300 dimensions, embeddings fix√©s) : recherche des meilleurs r√©glages avec Optuna + early stopping, meilleure F1 validation ‚âà0,74.
  - *GloVe* (embeddings 50‚Äì100 dimensions, certains entra√Æn√©s) : meilleure F1 validation ‚âà0,76, test global ‚âà0,75 de pr√©cision/‚âà0,74 de F1.
  Toutes les exp√©riences sont suivies dans MLflow et export√©es en packages pr√™ts √† servir.

## Mod√©lisation USE
Le notebook `Mes_notebooks/Model_3_USE/notebook_03.ipynb` teste l‚Äôencodeur de phrases Universal Sentence Encoder (USE) de Google pour transformer chaque tweet en vecteur dense avant de le passer √† un petit r√©seau de neurones.

- **Nettoyage l√©ger** : texte en minuscules, √©mojis/emotic√¥nes convertis en mots, suppression des liens/mentions/hashtags et des nombres, tout en conservant la ponctuation pour garder le ton.
- **Jeu de travail** : m√™me base √©quilibr√©e que les approches pr√©c√©dentes (8 000 tweets n√©gatifs et 8 000 positifs), d√©coup√©e en entra√Ænement/validation/test.
- **Encodage USE** : les tweets normalis√©s sont encod√©s une seule fois avec USE (512 dimensions), ce qui fournit directement une repr√©sentation ‚Äúsens‚Äù de la phrase, sans calculer de n‚Äëgrammes ni d‚Äôembeddings maison.
- **Mod√®le entra√Æn√©** : un petit r√©seau dense (dropout + couche cach√©e + sortie sigmo√Øde) est ajust√© avec Optuna pour choisir notamment la taille de la couche cach√©e et le taux de dropout, avec arr√™t anticip√© pour √©viter le surapprentissage.



## Mod√©lisation Distilbert
Le notebook `Mes_notebooks/Model_4_DISTILBERT/notebook_04.ipynb` exploite DistilBERT pour capter le contexte complet des phrases.

- **Nettoyage minimal** : on se contente d‚Äôenlever les espaces superflus, sans toucher √† la casse ni √† la ponctuation pour ne pas perdre d‚Äôindice de ton.
- **Jeu de travail √©largi** : 20 000 tweets n√©gatifs + 20 000 positifs (√©quilibr√©s), puis d√©coupe train/validation/test.
- **Pr√©paration texte** : normalisation l√©g√®re apr√®s le split, conversion en format `Dataset` Hugging Face, tokenisation avec le tokenizer DistilBERT.
- **Fine-tuning** : entra√Ænement de `distilbert-base-uncased-finetuned-sst-2-english` via la Trainer API avec suivi MLflow. Apr√®s une courte passe d‚Äôentra√Ænement, on obtient une F1 de validation autour de 0,83.
- **D√©ploiement optimis√©** : ajout d‚Äôune version quantifi√©e dynamiquement du mod√®le pour r√©duire la taille et acc√©l√©rer l‚Äôinf√©rence.

Le notebook `Mes_notebooks/notebook_06.ipynb` compl√®te l‚Äô√©valuation en lan√ßant une inf√©rence par mod√®le sauvegard√© et en mesurant le pic de m√©moire RSS pour comparer le co√ªt RAM des pipelines.

# BenchMarking des mod√®les

| Mod√®le                                | F1 validation | F1 test | Accuracy tweets difficiles | Commentaire rapide                                    |
|---------------------------------------|---------------|---------|----------------------------|-------------------------------------------------------|
| LogReg TF‚ÄëIDF (+ features simples)    | ‚âà0,73         | ‚âà0,74   | ‚âà0,70                      | Baseline l√©g√®re, facile √† servir                      |
| BiLSTM Word2Vec                       | ‚âà0,74         | ‚âà0,71   | ‚âà0,70                      | Capte le contexte, am√©liore l√©g√®rement la robustesse  |
| BiLSTM GloVe                          | ‚âà0,76         | ‚âà0,74   | ‚âà0,73                      | Meilleur √©quilibre BiLSTM, mais sensible au sarcasme  |
| USE + r√©seau dense                    | ‚âà0,78         | ‚âà0,79   | ‚âà0,78                      | Plus l√©ger que BiLSTM, robuste sur cas pi√©geux        |
| DistilBERT (version de base & quantiz√©) | ‚âà0,82       | ‚âà0,82   | ‚âà0,85                      | Meilleure pr√©cision et robustesse globale             |

D'apr√©s ce comparatif de performances, et avec le test de consommations fait dans le "notebook_06", DISTILBERT (quantiz√©) repr√©sente le meilleur compromis performance/consommation et le modele qu'on choisit pour le d√©ploiement.

# Architecture de l'application

## **API (FastAPI)**

![API docs](../Support_presentation/FastApi_docs.png)

  - **Endpoints** : `/predict` (retourne label + score apr√®s nettoyage du texte) et `/feedback` (remonte les retours utilisateurs avec le texte, le label attendu et un commentaire √©ventuel).

  - **Tests** : `tests/test_main.py` v√©rifie le pr√©traitement, le chargement du mod√®le, les r√©ponses des endpoints et la journalisation des feedbacks.

  - **Mise en ≈ìuvre du MLOps** : principes de test/trace/d√©ploiement automatis√©s.  
    - Principes : versionner code + mod√®les, tracer les exp√©riences, tester en continu, d√©ployer automatiquement.

    - Application :
      - GitHub Actions (`azure-webapp.yml`) installe les d√©pendances FastAPI, lance `pytest tests/test_main.py`, zippe l‚ÄôAPI + mod√®le ONNX/tokenizer, pousse l‚Äôarchive sur App Service et applique la commande de d√©marrage/`WEBSITES_PORT`.
      
      **Interface montrant les github actions**
      ![GithubActions](../Support_presentation/GithubActions_ui.png)

      - Suivi des retours via Azure Monitor/Application Insights (`configure_azure_monitor` + logs `prediction_feedback_reported` avec `custom_dimensions`).

      **Logs**
      ![Logs](../Support_presentation/AzureMonitor_logs.png)

      **Alertes**
      ![Alertes](../Support_presentation/AzureMonitor_alertes.png)
      
      - D√©ploiement de l‚ÄôAPI sur Azure App Service (ASP F1).

      **ASP dashboard**
      ![ASP](../Support_presentation/ASP_machine.png)

      - Tracking des exp√©riences et artefacts avec MLflow.

      **MLFLow lors du stockage des modelisations BITSLM**
      ![MLFLow](../Support_presentation/MLFLow_ui.png)

## **UI (Streamlit)**
  - Composants principaux :
      - Zone de saisie du tweet.
      - Affichage du r√©sultat de pr√©diction avec possibilit√© de signaler une erreur.
      - Historique des tweets analys√©s.

![Front UI](../Support_presentation/Front_UI.png)

  - Les appels API, la persistance d‚Äôhistorique et le feedback utilisateur sont couverts par `tests/test_streamlit_app.py`.


# Strat√©gie d'am√©lioration continue du mod√®le

- Boucler avec les retours utilisateurs : stocker les feedbacks (pr√©diction correcte/incorrecte) et r√©entra√Æner p√©riodiquement avec ces exemples difficiles pour r√©duire les erreurs sur le terrain (appliquer le concept du retraining du MLOps).

# Conclusion

Gr√¢ce aux diff√©rentes mod√®lisations , nous avons pu observer la diff√©rence entre les types de mod√®le en confirmant la sup√®riorit√© des transformers. La cha√Æne MLOps (tests, tracking MLflow, CI/CD, monitoring Azure, d√©ploiement sur App Service) permet de les faire √©voluer rapidement et de fa√ßon fiable.
Les gains futurs passeront par l‚Äôint√©gration continue des feedbacks terrain pour le reentrainement du modele, ainsi que d'autres options comme l'enrichissement des donn√©es.
Ce socle offre une base solide pour proposer des solution fiables √† des cas d'utilisation comme celui de notre client (AirParadis).
